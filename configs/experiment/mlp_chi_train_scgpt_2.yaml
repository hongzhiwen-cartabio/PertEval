# @package _global_

# run with `python src/train.py experiment=mlp_norman_train`

model_type: "mlp" # pick from mean, mlp or lr

defaults:
  - override /model: mlp
  - override /logger: wandb

total_genes: 15989 # 15989
emb_dim: 512 # 512 for geneformer, 512 for scGPT, 1280 for UCE, 3072 for scFoundation
hidden_dim: 256 # embed_dim / 2
mean_adjusted: false
save_dir: ${paths.data_dir}/${data.data_name}/pert_effects/${data.eval_pert}/pert_effect_pred_${data.fm}.pkl


data:
  data_name: "chi_data"
#  data_type: "scgpt"
  split: 0
  deg_eval: false
  eval_pert: null
  replicate: 0
  batch_size: 64
  fm: "scgpt"

trainer:
  max_epochs: 100
  accelerator: gpu
  devices: 1

callbacks:
  learning_rate_monitor:
    _target_: lightning.pytorch.callbacks.LearningRateMonitor
    logging_interval: 'epoch'

logger:
  wandb:
    tags: ["${model_type}", "${data.data_name}", "${data.fm}","split_${data.split}", "replicate_${data.replicate}", "hpo"]
    group: "${model_type}_${data.data_name}_${data.split}"
    project: "perturbench-local"

model:
  optimizer:
    _target_: torch.optim.AdamW
    _partial_: true
    lr: 5e-6
    weight_decay: 1e-8

  net:
    _target_: src.models.components.predictors.MLP
    in_dim: ${eval:'${emb_dim}*2'}
    # in_dim: ${emb_dim}

  scheduler:
    _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
    _partial_: true
    mode: 'min'
    factor: 0.1
    patience: 10
    min_lr: 5e-9

  data_name: "${data.data_name}"
  fm: "${data.fm}"
  split: "${data.split}"
